require 'nokogiri'
require 'rexml/document'
require 'find'
require 'net/http'

# A class to check links in the locally-produced HTML files
# for a clone of ruby/ruby:
#
# 1. Extracts links from the HTML files.
# 1. Verifies that the target page (local file or web page) exists.
# 1. Verifies that the URL fragment, if it exists, corresponds
#    to a linkable element on the target page.
# 1. Produces an HTML report file that:
#
#    - Gives full information about each broken link (page or fragment).
#    - Lists all off-site target pages.
#
class LinkChecker

  # All this was begun as a project to check links on the Ruby website,
  # and the reporting was implemented using REXML.
  # Later the project switched to checking links in files generated locally,
  # for which Nokogiri is the better choice.
  # Nokogiri could also do the reporting (if anyone cares to re-write it).
  include REXML

  # \Hash of \Page objects by path.
  attr_accessor :pages

  # \Hash of counts by name.
  attr_accessor :counts

  # \Array of (simple) paths to source pages (e.g., 'Array.html').
  attr_accessor :source_paths

  # Whether to output progress to $stdout.
  attr_accessor :verbose

  # Regexp to identify paths to be omitted.
  attr_accessor :omit_source_paths_regexp

  # Whether to omit off-site target pages.
  attr_accessor :omit_offsite_targets

  # Path to HTML directory.
  attr_accessor :html_dirpath

  # File path for the output report.
  attr_accessor :report_file_path

  # Returns a new \LinkChecker object:
  #
  # - +html_dirpath+: path to the local directory containing HTML files
  #   generated by RDoc.
  #
  #   These files may be created with an RDoc command
  #   (given in the ruby clone directory) such as:
  #
  #     rdoc --visibility=private --op html . # Note the trailing dot.
  #
  #  - +report_file_path+: path to the report file that will be written;
  #    defaults to <tt>'./Report.html'</tt>.
  #    It's best not to put this file into the file tree at +html_dirpath+.
  #
  # - +omit_source_paths_regexp+: \Regexp to specify paths to omit;
  #   defaults to +nil+ (omit no source paths).
  #
  #   In particular, the regexp may be given
  #   as <tt>/table_of_contents\.html/</tt> to omit the table of contents file.
  #
  # - +omit_offsite_targets+: whether to omit off-site targets:
  #
  #     - +false+ (the default): off-site target pages are visited, evaluated,
  #      and included in the report;
  #     - +true+: off-site target pages are not visited, evaluated, or reported on.
  #
  # - +verbose+: whether to output massive progress messages to $stdout.
  #
  def initialize(
    html_dirpath,
    report_file_path: './Report.html',
    omit_source_paths_regexp: nil,
    omit_offsite_targets: false,
    verbose: false
  )
    self.html_dirpath = html_dirpath
    self.report_file_path = report_file_path
    self.omit_source_paths_regexp = omit_source_paths_regexp
    self.omit_offsite_targets = omit_offsite_targets
    self.verbose = verbose
    self.pages = {}
    self.counts = {
      source_pages: 0,
      target_pages: 0,
      links_checked: 0,
      links_broken: 0,
    }
  end

  # Check links.
  #
  def check
    # All work is done in the HTML directory,
    # and that is where Report.htm will be put.
    counts[:start_time] = Time.now
    Dir.chdir(html_dirpath) do |dir|
      gather_source_paths
      create_source_pages
      create_target_pages
      verify_links
    end
    counts[:end_time] = Time.now
    report
  end

  # Gather paths to source HTML pages.
  def gather_source_paths
    paths = []
    puts 'Gathering source paths' if verbose
    paths = Find.find('.').select {|path| path.end_with?('.html') }
    paths = paths.select {|path| !path.match(self.omit_source_paths_regexp)} if
      self.omit_source_paths_regexp
    # Remove leading './'.
    self.source_paths = paths.map{|path| path.sub(%r[^\./], '')}
    # Filter.
    if verbose
      source_paths.each_with_index do |source_path, i|
        puts '- %4d %s' % [i, source_path]
      end
    end
    counts[:source_pages] = source_paths.size
    puts "Gathered #{source_paths.size} source paths" if verbose
  end

  # Create a source \Page object for each source path.
  # Gather its links and ids.
  def create_source_pages
    puts "Creating #{self.source_paths.size} source pages" if verbose
    self.source_paths.sort.each_with_index do |source_path, i|
      progress_s = LinkChecker.progress_s(i + 1, source_paths.size)
      puts "Creating source page #{source_path} #{progress_s}" if verbose
      source_page = Page.new(source_path, verbose, pages, counts)
      pages[source_path] = source_page
      source_text = File.read(source_path)
      doc = Nokogiri::HTML(source_text)
      source_page.gather_links(doc, omit_offsite_targets)
      source_page.gather_ids(doc)
      puts "Created source page #{progress_s}" if verbose
    end
    puts "Created #{pages.size} source pages" if verbose
  end

  # Create a target \Page object for each link
  # (unless already created as a source page).
  def create_target_pages
    doc = nil
    target_page_count = 0
    source_paths = pages.keys
    source_paths.each do |source_path|
      # Need for relative links to work.
      dirname = File.dirname(source_path)
      Dir.chdir(dirname) do
        source_page = pages[source_path]
        puts "Creating target pages for #{source_page.links.size} links in #{source_path}" if verbose
        source_page.links.each_with_index do |link, i|
          next if link.path.nil?
          link.puts(i) if verbose
          target_path = link.real_path
          if pages[target_path]
            puts "Page #{target_path} already created" if verbose
            target_page = pages[target_path]
          else
            if File.readable?(link.path)
              puts "Creating target page #{target_path}" if verbose
              target_page_count += 1
              target_page = Page.new(target_path, verbose, pages, counts)
              pages[target_path] = target_page
              target_text = File.read(link.path)
              doc = Nokogiri::HTML(target_text)
              target_page.gather_ids(doc)
              puts "Created target page #{target_path}" if verbose
            elsif LinkChecker.checkable?(link.path)
              puts "Creating target page #{target_path}" if verbose
              target_page_count += 1
              target_page = Page.new(target_path, verbose, pages, counts)
              pages[target_path] = target_page
              puts "Created target page #{target_path}" if verbose
              link.exception = fetch(link.path, target_page)
              link.valid_p = false if link.exception
            else
              puts "File not readable or checkable: #{target_path}" if verbose
            end
          end
          next if target_page.nil?
          if link.has_fragment? && target_page.ids.empty?
            doc || doc = Nokogiri::HTML(target_text)
            target_page.gather_ids(doc)
          end
        end
        puts "Created target pages for #{source_page.links.size} links in #{source_path}" if verbose
      end
    end
    puts "Created #{target_page_count} target pages" if verbose
    counts[:target_pages] = target_page_count
  end

  # Verify that each link target exists.
  def verify_links
    linking_pages = pages.select do |path, page|
      !page.links.empty?
    end
    puts "Checking links on #{linking_pages.size} pages" if verbose
    link_count = 0
    broken_count = 0
    linking_pages.each_pair do |path, page|
      puts "Checking #{page.links.size} links on page #{path}" if verbose
      link_count += page.links.size
      page.links.each_with_index do |link, i|
        if link.valid_p.nil? # Don't disturb if already set to false.
          target_page = pages[link.real_path]
          if target_page
            target_id = link.fragment
            link.valid_p = target_id.nil? || target_page.ids.include?(target_id)
          else
            link_valid_p = false
          end
        end
        link.puts(i) if verbose
        broken_count += 1 unless link.valid_p
      end
      puts "Checked #{page.links.size} links on page #{path}" if verbose
    end
    puts "Checked #{link_count} links on #{linking_pages.size} pages" if verbose
    counts[:links_checked] = link_count
    counts[:links_broken] = broken_count
  end

  # Fetch the page from the web and gather its ids into the target page.
  # Returns exception or nil.
  def fetch(url, target_page)
    puts "Begin fetch target page #{url}" if verbose
    puts "Getting return code for #{url}" if verbose
    code = 0
    exception = nil
    begin
      response =  Net::HTTP.get_response(URI(url))
      code = response.code.to_i
      target_page.code = code
      puts "Returned #{code} (#{response.class})" if verbose
    rescue => x
      puts "Raised #{x.class} #{x.message}" if verbose
      raise unless x.class.name.match(/^(Net|SocketError|IO::TimeoutError|Errno::)/)
      exception = LinkChecker::HttpResponseError.new(url, x)
    end
    puts "Got return code #{code} for #{url} " if verbose
    # Don't load if bad code, or no response, or if not html.
    if !code_bad?(code)
      if content_type_html?(response)
        doc = Nokogiri::HTML(response.body)
        target_page.gather_ids(doc)
      end
    else
      unless exception
        exception = LinkChecker::HttpStatusCodeError.new(url, code)
      end
    end
    puts "End fetch target page #{url}" if verbose
    exception
  end

  # Returns whether the code is bad (zero or >= 400).
  def code_bad?(code)
    return false if code.nil?
    (code == 0) || (code >= 400)
  end

  # Returns whether the response body should be HTML.
  def content_type_html?(response)
    return false unless response
    return false unless response['Content-Type']
    response['Content-Type'].match('html')
  end

  # Returns whether the path is offsite.
  def self.offsite?(path)
    path.start_with?('http')
  end

  # Returns the string fragment for the given path or ULR, or +nil+
  def self.get_fragment(s)
    a = s.split('#', 2)
    a.size == 2 ? a[1] : nil
  end

  # Returns a progress string giving a fraction and percentage.
  def self.progress_s(i, total)
    fraction_s = "#{i}/#{total}"
    percent_i = (i*100.0/total).round
    "(#{fraction_s}, #{percent_i}%)"
  end

  # Returns whether the path is checkable.
  def self.checkable?(path)
    return false unless path
    begin
      uri = URI(path)
      return ['http', 'https', nil].include?(uri.scheme)
    rescue
      return false
    end
  end

  # Class to represent a page.
  class Page

    attr_accessor :path, :type, :verbose, :pages, :counts, :code, :links, :ids, :dirname

    # Returns a new \Page object:
    #
    # - +path+: a path relative to the HTML directory (if on-site)
    #   or a URL (if off-site).
    # - +verbose+: whether to put progress message to $stdout.
    # - +pages+: hash of path/page pairs.
    # - +counts+: hash of counts.
    #
    def initialize(path, verbose, pages, counts)
      self.path = path
      self.verbose = verbose
      self.pages = pages
      self.counts = counts
      self.code = nil
      self.links = []
      self.ids = []
      self.dirname = File.dirname(path)
      self.dirname = self.dirname == '.' ? '' : dirname
    end

    # Gather links for the page:
    #
    # - +doc+: Nokogiri document to be parsed for links.
    # - +omit_offsite_targets+: whether to omit off-site targets.
    #
    def gather_links(doc, omit_offsite_targets)
      puts 'Gathering links' if verbose
      i = 0
      # The links are in the anchors.
      doc.search('a').each do |a|
        # Ignore pilcrow (paragraph character) and up-arrow.
        next if a.text == "\u00B6"
        next if a.text == "\u2191"

        href = a.attr('href')
        next if href.nil? or href.empty?
        next if LinkChecker.offsite?(href) && omit_offsite_targets
        next unless LinkChecker.checkable?(href)

        link = Link.new(href, a.text, dirname)
        next if link.path.nil? || link.path.empty?

        links.push(link)
        link.puts(i) if verbose
        i += 1
      end
      puts "Gathered #{i} links" if verbose
    end

    # Gather ids for the page.
    # +doc+ is the Nokogiri document to be parsed.
    def gather_ids(doc)
      # Don't do twice (some pages are both source and target).
      return unless ids.empty?

      # For off-site, gather all ids, regardless of element.
      if LinkChecker.offsite?(path)
        doc.xpath("//*[@id]").each do |element|
          id = element.attr('id')
          ids.push(id)
        end
        return
      end

      # We're on-site, which means that the page is RDoc-generated
      # and we know what to expect.
      # In theory, an author can link to any element that has an attribute :id.
      # In practice, gathering all such elements is very time-consuming.
      # These are the elements currently linked to:
      #
      # - body
      # - a
      # - div
      # - dt
      # - h*
      #
      # We can add more as needed (i.e., if/when we have actual broken links).
      puts 'Gathering potential link targets' if verbose

      # body element has 'top', which is a link target.
      body = doc.at('//body')
      id = body.attribute('id')
      ids.push(id) if id

      # Some ids are in the as (anchors).
      body.search('a').each do |a|
        id = a.attr(id)
        ids.push(id) if id
      end

      # Method ids are in divs, but gather only method-detail divs.
      body.search('div').each do |div|
        class_ = div.attr('class')
        next if class_.nil?
        next unless class_.match('method-')
        id = div.attr('id')
        ids.push(id) if id
      end

      # Constant ids are in dts.
      body.search('dt').each do |dt|
        id = dt.attr('id')
        ids.push(id) if id
      end

      # Label ids are in headings.
      %w[h1 h2 h3 h4 h5 h6].each do |tag|
        body.search(tag).each do |h|
          id = h.attr('id')
          ids.push(id) if id
        end
      end
      if verbose
        ids.each_with_index do |id, i|
          puts '%4d %s' % [i, id]
        end
      end
      puts "Gathered #{ids.size} potential link targets" if verbose

    end

  end

  # Class to represent a link.
  class Link

    attr_accessor :href, :text, :dirname, :path, :fragment, :valid_p, :real_path, :exception

    # Returns a new \Link object:
    #
    # - +href+: attribute href from anchor element.
    # - +text+: attribute text from anchor element.
    # - +dirname+: directory path of the linking page.
    #
    # TODO: accept the anchor element, instead of its href and text.
    def initialize(href, text, dirname)
      self.href = href
      self.text = text
      self.dirname = dirname
      path, fragment = href.split('#', 2)
      self.path = path
      self.fragment = fragment
      self.valid_p = nil
      self.real_path = make_real_path(dirname, path)
      self.exception = nil
    end

    # Return the real (not relative) path of the link.
    def make_real_path(dirname, path)
      # Trim single dot.
      return path.sub('./', '') if path.start_with?('./')
      return path if dirname.nil? || dirname.empty?

      # May have one or more leading '../'.
      up_dir = '../'
      levels = path.scan(/(?=#{up_dir})/).count
      dirs = dirname.split('/')
      if levels == 0
        dirs.empty? ? path : File.join(dirname, path)
      else
        # Remove leading '../' elements.
        path = path.gsub(%r[\.\./], '')
        # Remove the corresponding parts of dirname.
        dirs.pop(levels)
        return path if dirs.empty?
        dirname = dirs.join('/')
        File.join(dirname, path)
      end
    end

    # Returns whether the link has a fragment.
    def has_fragment?
      fragment ? true : false
    end

    # Puts link info onto $stdout.
    def puts(i)
      $stdout.puts <<EOT
Link #{i}:
  Href:      #{href}
  Text:      #{text}
  Path:      #{path}
  Fragment:  #{fragment}
  Valid:     #{valid_p}
  Real path: #{real_path}
  Dirname:   #{dirname}
EOT
    end
  end

  # Generate the report; +checker+ is the \LinkChecker object.
  def report

    doc = Document.new('')
    root = doc.add_element(Element.new('root'))

    head = root.add_element(Element.new('head'))
    title = head.add_element(Element.new('title'))
    title.text = 'LinkChecker Report'
    style = head.add_element(Element.new('style'))
    style.text = <<EOT
*        { font-family: sans-serif }
.data    { font-family: courier }
.center  { text-align: center }
.good    { color: rgb(  0,  97,   0); background-color: rgb(198, 239, 206) } /* Greenish */
.iffy    { color: rgb(156, 101,   0); background-color: rgb(255, 235, 156) } /* Yellowish */
.bad     { color: rgb(156,   0,   6); background-color: rgb(255, 199, 206) } /* Reddish */
.neutral { color: rgb(  0,   0,   0); background-color: rgb(217, 217, 214) } /* Grayish */
EOT

    body = root.add_element(Element.new('body'))
    h1 = body.add_element(Element.new('h1'))
    h1.text = 'LinkChecker Report'

    self.add_summary(body)
    self.add_broken_links(body)
    self.add_offsite_links(body) unless omit_offsite_targets
    doc.write(File.new(report_file_path, 'w'), 2)
  end

  def add_summary(body)
    h2 = body.add_element(Element.new('h2'))
    h2.text = 'Summary'

    # Parameters table.
    data = []
    [
      :html_dirpath,
      :report_file_path,
      :omit_source_paths_regexp,
      :omit_offsite_targets,
      :verbose
    ].each do |sym|
      value = send(sym).inspect
      row = {sym => :label, value => :good}
      data.push(row)
    end
    table2(body, data, 'Parameters')
    body.add_element(Element.new('p'))

    # Times table.
    elapsed_time = counts[:end_time] - counts[:start_time]
    seconds = elapsed_time % 60
    minutes = (elapsed_time / 60) % 60
    hours = (elapsed_time/3600)
    elapsed_time_s = "%2.2d:%2.2d:%2.2d" % [hours, minutes, seconds]
    format = "%Y-%m-%d-%a-%H:%M:%S"
    start_time_s = counts[:start_time].strftime(format)
    end_time_s = counts[:end_time].strftime(format)
    data = [
      {'Start Time' => :label, start_time_s => :good},
      {'End Time' => :label, end_time_s => :good},
      {'Elapsed Time' => :label, elapsed_time_s => :good},
    ]
    table2(body, data, 'Times')
    body.add_element(Element.new('p'))

    # Counts.
    data = [
      {'Source Pages' => :label, counts[:source_pages] => :good},
      {'Target Pages' => :label, counts[:target_pages] => :good},
      {'Links Checked' => :label, counts[:links_checked] => :good},
      {'Links Broken' => :label, counts[:links_broken] => :bad},
    ]
    table2(body, data, 'Counts')
    body.add_element(Element.new('p'))

  end

  def add_broken_links(body)
    h2 = body.add_element(Element.new('h2'))
    h2.text = 'Broken Links by Source Page'

    if counts[:links_broken] == 0
      p = body.add_element('p')
      p.text = 'None.'
      return
    end

    ul = body.add_element(Element.new('ul'))
    li = ul.add_element(Element.new('li'))
    li.text = 'Href: the href of the anchor element.'
    li = ul.add_element(Element.new('li'))
    li.text = 'Text: the text of the anchor element.'
    li = ul.add_element(Element.new('li'))
    li.text = <<EOT
Path: the URL or path of the link (not including the fragment).
For an on-site link, an abbreviated path is given;
for an off-site link, the full URL is given.
If the path is reddish, the page was not found.
EOT
    li = ul.add_element(Element.new('li'))
    li.text = <<EOT
Fragment: the fragment of the link.
If the fragment is reddish, fragment was not found.
EOT

    pages.each_pair do |path, page|
      broken_links = page.links.select {|link| !link.valid_p }
      next if broken_links.empty?

      h3 = body.add_element(Element.new('h3'))
      a = Element.new('a')
      a.text = path
      a.add_attribute('href', path)
      h3.add_element(a)

      broken_links.each do |link|
        data = []
        # Text, URL, fragment
        a = Element.new('a')
        a.text = link.href
        a.add_attribute('href', link.href)
        data.push({'Href' => :label, a => :bad})
        data.push({'Text' => :label, link.text => :good})
        fragment_p = !link.fragment.nil?
        class_ = fragment_p ? :good : :bad
        data.push({'Path' => :label, link.real_path => class_})
        class_ = fragment_p ? :bad : :good
        data.push({'Fragment' => :label, link.fragment => class_})
        if link.exception
          data.push({'Exception' => :label, link.exception.class => :bad})
          data.push({'Message' => :label, link.exception.message => :bad})
        end
        table2(body, data)
        body.add_element(Element.new('p'))
      end
    end

  end

    def add_offsite_links(body)
    h2 = body.add_element(Element.new('h2'))
    h2.text = 'Off-Site Links by Source Page'
    pages.each_pair do |path, page|
      offsite_links = page.links.select do |link|
        LinkChecker.offsite?(link.href)
      end
      next if offsite_links.empty?

      h3 = body.add_element(Element.new('h3'))
      a = Element.new('a')
      a.text = path
      a.add_attribute('href', path)
      h3.add_element(a)

      offsite_links.each do |link|
        data = []
        # Text, URL, fragment
        a = Element.new('a')
        a.text = link.href
        a.add_attribute('href', link.href)
        class_ = link.valid_p ? :good : :bad
        data.push({'Href' => :label, a => class_})
        data.push({'Text' => :label, link.text => :good})
        table2(body, data)
        body.add_element(Element.new('p'))
      end
    end
  end

  Classes = {
    label: 'label center neutral',
    good: 'data center good',
    iffy: 'data center iffy',
    bad: 'data center bad',
  }

  def table2(parent, data, title = nil)
    data = data.dup
    table = parent.add_element(Element.new('table'))
    if title
      tr = table.add_element(Element.new('tr)'))
      th = tr.add_element(Element.new('th'))
      th.add_attribute('colspan', 2)
      if title.kind_of?(REXML::Element)
        th.add_element(title)
      else
        th.text = title
      end
    end
    data.each do |row_h|
      label, label_class, value, value_class = row_h.flatten
      tr = table.add_element(Element.new('tr'))
      td = tr.add_element(Element.new('td'))
      td.text = label
      td.add_attribute('class', Classes[label_class])
      td = tr.add_element(Element.new('td'))
      if value.kind_of?(REXML::Element)
        td.add_element(value)
      else
        td.text = value
      end
      td.add_attribute('class', Classes[value_class])
    end
  end

  class Error; end

  class HttpResponseError < Error

    attr_accessor :url, :x

    def initialize(url, x)
      self.url = url
      self.x = x
    end

    def message
      <<EOT
#{self.class.name}:
An exception was raised when checking page availability with Net::HTTP:
  Url: #{url}
  Class: #{x.class}
  Message: #{x.message}
EOT
    end

  end

  class HttpStatusCodeError < Error

    attr_accessor :url, :code

    def initialize(url, code)
      self.url = url
      self.code = code
    end

    def message
      <<EOT
#{self.class.name}:
  The return code for the page was not 200:
    Url: #{url}
    Return code: #{code}
EOT
    end

  end

end

if $0 == __FILE__
  checker = LinkChecker.new(
    '../ruby/html/',
    report_file_path: './Report.html',
    omit_source_paths_regexp: /table_of_contents\.html/,
    omit_offsite_targets: false,
    verbose: false
  )
  checker.check
end
